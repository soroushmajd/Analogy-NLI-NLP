{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TRAgqwO8e0a0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext.vocab as vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dadmatools "
      ],
      "metadata": {
        "id": "SEi7csrsj1GV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e85c2286-9389-426e-926c-1b06bde905f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dadmatools\n",
            "  Downloading dadmatools-1.5.2-py3-none-any.whl (862 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (1.13.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from dadmatools) (3.7)\n",
            "Collecting transformers>=4.9.1\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated==1.2.6\n",
            "  Downloading Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting pytorch-transformers>=1.1.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (0.12.1.post1)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (0.8.10)\n",
            "Collecting sklearn>=0.0\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hyperopt>=0.2.5\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (4.4.0)\n",
            "Collecting h5py>=3.3.0\n",
            "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting NERDA\n",
            "  Downloading NERDA-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting pyconll>=3.1.0\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (3.4.4)\n",
            "Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (3.6.0)\n",
            "Collecting py7zr>=0.17.2\n",
            "  Downloading py7zr-0.20.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.3\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting supar==1.1.2\n",
            "  Downloading supar-1.1.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated==1.2.6->dadmatools) (1.14.1)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.3/691.3 KB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from supar==1.1.2->dadmatools) (0.3.6)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.3->dadmatools) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.3->dadmatools) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.3->dadmatools) (1.21.6)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from folium>=0.2.1->dadmatools) (2.11.3)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from folium>=0.2.1->dadmatools) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown>=4.3.1->dadmatools) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown>=4.3.1->dadmatools) (3.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown>=4.3.1->dadmatools) (4.6.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.6.0->dadmatools) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.6.0->dadmatools) (1.7.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.16.0)\n",
            "Collecting texttable\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyppmd<1.1.0,>=0.18.1\n",
            "  Downloading pyppmd-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.7/139.7 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr>=0.17.2->dadmatools) (5.4.8)\n",
            "Collecting pybcj>=0.6.0\n",
            "  Downloading pybcj-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1\n",
            "  Downloading inflate64-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4\n",
            "  Downloading pyzstd-0.15.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.0/379.0 KB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex>=3.6.6\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.0.9\n",
            "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.67-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (1.10.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (8.1.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.1->dadmatools) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.9.1->dadmatools) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from NERDA->dadmatools) (1.3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->dadmatools) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->dadmatools) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->dadmatools) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->dadmatools) (0.7.9)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.67\n",
            "  Downloading botocore-1.29.67-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->NERDA->dadmatools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->NERDA->dadmatools) (2022.7.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from stanza->supar==1.1.2->dadmatools) (3.19.6)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn, progressbar, sacremoses, emoji\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=4378ea9a80490ec46081c433053d43142bf2352bc1cd14f94dd7014038993a69\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12081 sha256=0b6c182c9a71a5e45b38770da84dbaab591813dc9a1c68a4706a0c193c25e885\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/67/ed/d84123843c937d7e7f5ba88a270d11036473144143355e2747\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=8d3a9b79bdd65f2d482ed7b9d5bfb310259ca6338f0378a0a7a3203533ff180d\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=6e9ebe5e1da724c5758b5ddb2dba462e558988a30000150ba9aab07a1ef80992\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built sklearn progressbar sacremoses emoji\n",
            "Installing collected packages: tokenizers, tf-estimator-nightly, texttable, sklearn, sentencepiece, py4j, progressbar, brotli, urllib3, segtok, sacremoses, pyzstd, pyppmd, pycryptodomex, pyconll, pybcj, multivolumefile, jmespath, inflate64, html2text, h5py, emoji, Deprecated, conllu, py7zr, hyperopt, botocore, stanza, s3transfer, huggingface-hub, bpemb, transformers, boto3, supar, pytorch-transformers, NERDA, dadmatools\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "Successfully installed Deprecated-1.2.6 NERDA-1.0.0 boto3-1.26.67 botocore-1.29.67 bpemb-0.3.4 brotli-1.0.9 conllu-4.5.2 dadmatools-1.5.2 emoji-2.2.0 h5py-3.8.0 html2text-2020.1.16 huggingface-hub-0.12.0 hyperopt-0.2.7 inflate64-0.3.1 jmespath-1.0.1 multivolumefile-0.2.3 progressbar-2.5 py4j-0.10.9.7 py7zr-0.20.2 pybcj-1.0.1 pyconll-3.1.0 pycryptodomex-3.17 pyppmd-1.0.0 pytorch-transformers-1.2.0 pyzstd-0.15.3 s3transfer-0.6.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.97 sklearn-0.0.post1 stanza-1.4.2 supar-1.1.2 texttable-1.6.7 tf-estimator-nightly-2.8.0.dev2021122109 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dadmatools"
      ],
      "metadata": {
        "id": "ubUfHzoe3qaW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7J29yRfQlUaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73816bc-1098-4ae6-e4d2-38813c4c5c0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4396422 sha256=9d1e0e9f1fed852dd732cd795267a587c3f810c1f75249030734c46fd985ab24\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utility"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZbH_GS7o2zR",
        "outputId": "86ba802b-fa0d-4320-a02a-e58c212d71bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utility\n",
            "  Downloading utility-1.0.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utility\n",
            "  Building wheel for utility (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utility: filename=utility-1.0-py3-none-any.whl size=3833 sha256=8f57dc0592cee23cd9bf0666caf0ac51065ef502c2929cfadb40e34bd7813622\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/88/f4/d0334bff483f781913d511756d750e37a1dc44e79e38893d41\n",
            "Successfully built utility\n",
            "Installing collected packages: utility\n",
            "Successfully installed utility-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/Test_file.txt\", engine='python',encoding='utf-8', error_bad_lines=False)\n",
        "\n",
        "df.columns = [\"cat\", \"first\", \"second\", \"third\",\"fourth\"]\n",
        "\n",
        "print(\"The size of the analogy test set is : {}\".format(len(df)))\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "H8tf1YwbpBWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5051d9e-5439-4d81-c61a-872e7dff26de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the analogy test set is : 129\n",
            "    cat     first   second      third   fourth\n",
            "0  city     اهواز  خوزستان        قشم  هرمزگان\n",
            "1  city     شیراز     فارس     دامغان    سمنان\n",
            "2  city       رشت    گیلان  اسلام‌شهر    تهران\n",
            "3  city  بندرعباس  هرمزگان      ملارد    تهران\n",
            "4  city      اراک    مرکزی     مریوان  کردستان\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec"
      ],
      "metadata": {
        "id": "MTxzQC64RA0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dadmatools\n",
        "from dadmatools.embeddings import get_embedding, get_all_embeddings_info, get_embedding_info\n",
        "embedding_info = get_embedding_info('word2vec-conll')\n",
        "word2vec_embedding = get_embedding('word2vec-conll')\n",
        "vocab = word2vec_embedding.get_vocab()\n"
      ],
      "metadata": {
        "id": "3VcAiqo5kapZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36fb677-59de-4230-9723-c072b68fff7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61.zip: 100%|██████████| 697M/697M [00:31<00:00, 23.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def find_cosine_similarity(vector1, vector2):\n",
        "    dot = np.dot(vector1,vector2)\n",
        "    norm1 = np.sqrt(np.sum(vector1**2))\n",
        "    norm2 = np.sqrt(np.sum(vector2**2))\n",
        "    cosine_sim = dot/(norm1)/norm2\n",
        "    \n",
        "    return cosine_sim"
      ],
      "metadata": {
        "id": "GbhB2wh_meLg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_word_analogy(a, b, c, embedding):\n",
        "    # Get the word embeddings\n",
        "    emb_a, emb_b, emb_c = word2vec_embedding[a], word2vec_embedding[b], word2vec_embedding[c]\n",
        "    \n",
        "    # Get the nearest 100000 words to word_c\n",
        "    nearest = word2vec_embedding.top_nearest(c, 50)\n",
        "    word_list = [word[0] for word in nearest]\n",
        "    max_similarity = -100000\n",
        "    chosen_word = None\n",
        "\n",
        "    # Search for word_d in the word vector set\n",
        "    for word in word_list:\n",
        "        # Skip input words\n",
        "        if word in [a, b, c]:\n",
        "            continue\n",
        "\n",
        "        # Compute cosine similarity between vectors\n",
        "        sim = find_cosine_similarity(emb_b - emb_a, word2vec_embedding[word] - emb_c)\n",
        "        \n",
        "        # Update chosen word if similarity is higher\n",
        "        if sim > max_similarity:\n",
        "            max_similarity = sim\n",
        "            chosen_word = word\n",
        "\n",
        "    return chosen_word    "
      ],
      "metadata": {
        "id": "tlRBlFzTsuXX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec accuracy"
      ],
      "metadata": {
        "id": "-ZalI-5g45kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"city\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (9):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "                   \n",
        "num = y[\"cat\"].count()      \n",
        "print(accuracy/num)      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPj3l_9SU2TM",
        "outputId": "0ae7d53f-3529-429f-eeb0-43ed986cf586"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "('اهواز', 'خوزستان', 'قشم')\n",
            "غارعلیصدر\n",
            "1\n",
            "('شیراز', 'فارس', 'دامغان')\n",
            "گناوه\n",
            "2\n",
            "3\n",
            "('بندرعباس', 'هرمزگان', 'ملارد')\n",
            "ساوجبلاغ\n",
            "4\n",
            "('اراک', 'مرکزی', 'مریوان')\n",
            "شیبکوه\n",
            "5\n",
            "6\n",
            "('سنندج', 'کردستان', 'آبادان')\n",
            "خوزستان\n",
            "7\n",
            "('گرگان', 'گلستان', 'قشم')\n",
            "تورارمنستان\n",
            "8\n",
            "0.1111111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"capital\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (9,19):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "                \n",
        "num = y[\"cat\"].count()      \n",
        "print(accuracy/num)      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bd5461-f5ed-4e37-f3aa-d1ba71624748",
        "id": "c4eJZzoGYnYi"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "('روسیه', 'مسکو', 'هلند')\n",
            "آمستردام\n",
            "10\n",
            "11\n",
            "('ایران', 'تهران', 'اسپانیا')\n",
            "مارسی\n",
            "12\n",
            "('ایران', 'تهران', 'افغانستان')\n",
            "هرات\n",
            "13\n",
            "('بریتانیا', 'لندن', 'فنلاند')\n",
            "سنگاپور\n",
            "14\n",
            "('تایلند', 'بانکوک', 'چین')\n",
            "شانگهای\n",
            "15\n",
            "('مصر', 'قاهره', 'چین')\n",
            "شانگهای\n",
            "16\n",
            "('اندونزی', 'جاکارتا', 'اتریش')\n",
            "بوداپست\n",
            "17\n",
            "('عراق', 'بغداد', 'بلغارستان')\n",
            "لیماسول\n",
            "18\n",
            "('آلمان', 'برلین', 'اسپانیا')\n",
            "مارسی\n",
            "0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"currency\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (19,29):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "                \n",
        "num = y[\"cat\"].count()      \n",
        "print(accuracy/num)      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jths3p-RYqds",
        "outputId": "c6073834-fd25-4809-cc1c-4b77c8a8660f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "('سوئد', 'کرون', 'امارات')\n",
            "فجیره\n",
            "20\n",
            "('سوئیس', 'فرانک', 'تاجیکستان')\n",
            "پنجکنت\n",
            "21\n",
            "('رومانی', 'لئو', 'بریتانیا')\n",
            "روتشیلد\n",
            "22\n",
            "('ترکیه', 'لیره', 'افغانستان')\n",
            "آنکشور\n",
            "23\n",
            "('چین', 'یوان', 'عراق')\n",
            "موصل\n",
            "24\n",
            "('اروپا', 'یورو', 'ایران')\n",
            "85053\n",
            "25\n",
            "('اروپا', 'یورو', 'هند')\n",
            "ماکاو\n",
            "26\n",
            "('ژاپن', 'ین', 'اروپا')\n",
            "یوفا/لیگ\n",
            "27\n",
            "('سوئیس', 'فرانک', 'کنیا')\n",
            "سقطرا\n",
            "28\n",
            "('عربستان', 'ریال', 'امارات')\n",
            "-یونان\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"family\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (29,39):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "            \n",
        "num = y[\"cat\"].count()      \n",
        "print(accuracy/num)      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgcgUafBYqj9",
        "outputId": "29f877bf-6b5f-4694-e790-71210ac56b16"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "('برادر', 'خواهر', 'پدر')\n",
            "شوهر\n",
            "30\n",
            "('برادرها', 'خواهرها', 'شوهر')\n",
            "برادرشوهر\n",
            "31\n",
            "('پدربزرگ', 'مادربزرگ', 'برادر')\n",
            "چقلی\n",
            "32\n",
            "33\n",
            "('داماد', 'عروس', 'برادرها')\n",
            "جداشوم\n",
            "34\n",
            "('شوهر', 'عیال', 'دایی')\n",
            "پنجعلی\n",
            "35\n",
            "('شاه', 'ملکه', 'برادر')\n",
            "خواهر\n",
            "36\n",
            "('آقا', 'خانم', 'آقاجون')\n",
            "چشممم\n",
            "37\n",
            "('عمو', 'عمه', 'پدربزرگ')\n",
            "خواهر\n",
            "38\n",
            "('دایی', 'خاله', 'پسردایی')\n",
            "دخترخاله\n",
            "0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_thirdperson\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (39,49):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()      \n",
        "print(accuracy/num)      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mX0b1ULYq4M",
        "outputId": "b3f05474-792e-4c55-c6c8-0587c96cbdd8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "('برید', 'بریدند', 'رقصید')\n",
            "رقصیدند\n",
            "40\n",
            "('پرید', 'پریدند', 'آمد')\n",
            "آمدند\n",
            "41\n",
            "42\n",
            "('شد', 'شدند', 'بوسید')\n",
            "ایستاد\n",
            "43\n",
            "('کرد', 'کردند', 'آمد')\n",
            "آمدند\n",
            "44\n",
            "('گفت', 'گفتند', 'رقصید')\n",
            "رقصیدند\n",
            "45\n",
            "('توانست', 'توانستند', 'یافت')\n",
            "یافتند\n",
            "46\n",
            "('پخت', 'پختند', 'نامید')\n",
            "نامیدند\n",
            "47\n",
            "('رسید', 'رسیدند', 'شد')\n",
            "شدند\n",
            "48\n",
            "('خواست', 'خواستند', 'آمد')\n",
            "آمدند\n",
            "0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_past\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (49,59):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-RhFSqQYq88",
        "outputId": "093e706e-36bf-43f5-e0ab-26add069b962"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "('بریدن', 'برید', 'دمیدن')\n",
            "جذبهٔ\n",
            "50\n",
            "('تازیدن', 'تاخت', 'دادن')\n",
            "بدهند\n",
            "51\n",
            "('شدن', 'شد', 'نگریستن')\n",
            "بنگرید\n",
            "52\n",
            "('گفتن', 'گفت', 'رقصیدن')\n",
            "بازدیدلباس\n",
            "53\n",
            "('گفتن', 'گفت', 'خواستن')\n",
            "*بالاخره\n",
            "54\n",
            "('توانستن', 'توانست', 'رسیدن')\n",
            "یلبد\n",
            "55\n",
            "('رفتن', 'رفت', 'زدن')\n",
            "زد\n",
            "56\n",
            "('رفتن', 'رفت', 'افزودن')\n",
            "intervento\n",
            "57\n",
            "('گرفتن', 'گرفت', 'توانستن')\n",
            "آرامشمان\n",
            "58\n",
            "('خواستن', 'خواست', 'فرمودن')\n",
            "جناب\n",
            "0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_firstperson\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (59,69):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)     \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si3GEyXhYrCV",
        "outputId": "7c99cd7a-5a6d-44d7-8a30-5628e820f5b0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n",
            "('دادم', 'دادیم', 'یافتم')\n",
            "یافتیم\n",
            "60\n",
            "('زدم', 'زدیم', 'نامیدم')\n",
            "وخدایت\n",
            "61\n",
            "('نوشتم', 'نوشتیم', 'گفتم')\n",
            ".خندیدم\n",
            "62\n",
            "('دانستم', 'دانستیم', 'بلعیدم')\n",
            "خورد.خیلی\n",
            "63\n",
            "('گردیدم', 'گردیدیم', 'یافتم')\n",
            "یافتیم\n",
            "64\n",
            "('گردیدم', 'گردیدیم', 'بوسیدم')\n",
            "لیسید\n",
            "65\n",
            "('گذاشتم', 'گذاشتیم', 'کردم')\n",
            "کردم.چون\n",
            "66\n",
            "('گذاشتم', 'گذاشتیم', 'ماندم')\n",
            "ماندست\n",
            "67\n",
            "('نگریستم', 'نگریستیم', 'رقصیدم')\n",
            ".جوووون\n",
            "68\n",
            "('یافتم', 'یافتیم', 'دمیدم')\n",
            "دمیدیم\n",
            "0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10y= df[df[\"cat\"]==\"gram_adj2adv\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (69,79):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)     \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP08up8nYrHJ",
        "outputId": "c3efdb43-7293-4dd8-90f6-84d0e78ebdea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n",
            "('مجدد', 'مجددا', 'مخصوص')\n",
            "تزئئین\n",
            "70\n",
            "('عمده', 'عمدتا', 'صمیمی')\n",
            "صمیمت\n",
            "71\n",
            "('دقیق', 'دقیقا', 'عادل')\n",
            "احمدی\n",
            "72\n",
            "('غالب', 'غالبا', 'جدا')\n",
            "وجابجا\n",
            "73\n",
            "74\n",
            "('متاسف', 'متاسفانه', 'قبل')\n",
            "پس\n",
            "75\n",
            "('خوشبخت', 'خوشبختانه', 'مجدد')\n",
            "مجدداً\n",
            "76\n",
            "('صمیمی', 'صمیمانه', 'صادق')\n",
            "اکرم\n",
            "77\n",
            "('آزاد', 'آزادانه', 'متاسف')\n",
            "سرخورده\n",
            "78\n",
            "('آگاه', 'آگاهانه', 'سرسخت')\n",
            "میلیتاریسم\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_noun2adv\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (79,89):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)     \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9C_mEhvYrR8",
        "outputId": "67df76a0-7bfd-4385-9fe6-04cb1629048c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79\n",
            "('اتفاق', 'اتفاقا', 'شب')\n",
            "سرمایادتون\n",
            "80\n",
            "('حتم', 'حتما', 'اساس')\n",
            "طبق\n",
            "81\n",
            "('اصل', 'اصلا', 'انسان')\n",
            ".انسان\n",
            "82\n",
            "('عمل', 'عملا', 'اجبار')\n",
            "اجباری\n",
            "83\n",
            "('اجبار', 'اجبارا', 'کودک')\n",
            "فتوزویا\n",
            "84\n",
            "('دائم', 'دائما', 'اتفاق')\n",
            "اتفاقها\n",
            "85\n",
            "('طبع', 'طبعا', 'سوم')\n",
            "دوم\n",
            "86\n",
            "('نسبت', 'نسبتا', 'اصل')\n",
            "romanson_خرید\n",
            "87\n",
            "('اصول', 'اصولا', 'دوم')\n",
            "اول\n",
            "88\n",
            "('اساس', 'اساسا', 'نسبت')\n",
            "نصبت\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_antonym\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (89,99):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_il1Y_gUYrXj",
        "outputId": "308e347a-a06a-4109-9fac-fc477561b69b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89\n",
            "('خشنود', 'ناخشنود', 'موفق')\n",
            "ناموفق\n",
            "90\n",
            "('درست', 'نادرست', 'پخته')\n",
            "زیتونهای\n",
            "91\n",
            "('مناسب', 'نامناسب', 'همزمان')\n",
            "متعاقب\n",
            "92\n",
            "('مربوط', 'نامربوط', 'منسجم')\n",
            "وسنجیده\n",
            "93\n",
            "('بینا', 'نابینا', 'درست')\n",
            "کیکام\n",
            "94\n",
            "('کارآمد', 'ناکارآمد', 'هنجار')\n",
            "ناموجه\n",
            "95\n",
            "('راضی', 'ناراضی', 'مطلوب')\n",
            "نامطلوب\n",
            "96\n",
            "('مطلوب', 'نامطلوب', 'مناسب')\n",
            "نامناسب\n",
            "97\n",
            "('پیدا', 'ناپیدا', 'متعارف')\n",
            "مکیل\n",
            "98\n",
            "('مساعد', 'نامساعد', 'درست')\n",
            "نمیپزه\n",
            "0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_comparative\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (99,109):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdcy-L64Yrc-",
        "outputId": "fb426416-4b35-4765-f95b-86c0b52b5522"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n",
            "('سریع', 'سریعتر', 'بلند')\n",
            "بلندتر\n",
            "100\n",
            "('بزرگ', 'بزرگتر', 'شدید')\n",
            "شدیدتر\n",
            "101\n",
            "('مهم', 'مهمتر', 'بارز')\n",
            "لحاظ\n",
            "102\n",
            "('به', 'بهتر', 'سرد')\n",
            "گرمتر\n",
            "103\n",
            "('واضح', 'واضحتر', 'بیش')\n",
            "معمولبسیار\n",
            "104\n",
            "('کم', 'کمتر', 'بد')\n",
            "بدتر\n",
            "105\n",
            "('بالا', 'بالاتر', 'به')\n",
            "اجازاه\n",
            "106\n",
            "('معروف', 'معروفتر', 'مشهور')\n",
            "وسرشناس\n",
            "107\n",
            "('بلند', 'بلندتر', 'نزدیک')\n",
            "نزدیکتر\n",
            "108\n",
            "('بد', 'بدتر', 'مهم')\n",
            "مهمتر\n",
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_nationality\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (109,118):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbteZqBcim2m",
        "outputId": "5c250a44-f5dc-4046-9184-4f63b33b3e01"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n",
            "('اتریش', 'اتریشی', 'اسپانیا')\n",
            "والنسیای\n",
            "110\n",
            "('سوئد', 'سوئدی', 'فنلاند')\n",
            "فرانسه\n",
            "111\n",
            "('بحرین', 'بحرینی', 'عراق')\n",
            "آمرلی\n",
            "112\n",
            "('کویت', 'کویتی', 'سوریه')\n",
            "تروریست\n",
            "113\n",
            "('مصر', 'مصری', 'کرواسی')\n",
            "برزیل\n",
            "114\n",
            "('مصر', 'مصری', 'فرانسه')\n",
            "آلمانی\n",
            "115\n",
            "('برزیل', 'برزیلی', 'آمریکا')\n",
            "آمریکایی\n",
            "116\n",
            "('برزیل', 'برزیلی', 'انگلستان')\n",
            "لوباویچ\n",
            "117\n",
            "('روسیه', 'روسی', 'یونان')\n",
            "یونانی\n",
            "0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[df[\"cat\"]==\"gram_plural\"]\n",
        "first = y[\"first\"]\n",
        "second = y[\"second\"]\n",
        "third = y[\"third\"]\n",
        "fourth = y[\"fourth\"]\n",
        "accuracy = 0\n",
        "for i in range (119,128):\n",
        "    print(i)\n",
        "    inputs = [(first[i], second[i], third[i])]\n",
        "\n",
        "    for input in inputs:\n",
        "        if(input[0] in vocab and input[1] in vocab and input[2] in vocab):\n",
        "            word4 = find_word_analogy(*input, word2vec_embedding)\n",
        "            print (input)\n",
        "            print (word4)\n",
        "            if (word4==fourth[i]):\n",
        "                accuracy = accuracy + 1\n",
        "             \n",
        "num = y[\"cat\"].count()   \n",
        "print(accuracy/num)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QCVzHlmituW",
        "outputId": "a9a345d1-6d84-4882-b90d-65bc01a5bca6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119\n",
            "('تالیف', 'تالیفات', 'مخلوق')\n",
            "نعمتهاى\n",
            "120\n",
            "('اثر', 'اثرات', 'ارزش')\n",
            "gdp\n",
            "121\n",
            "('انقلابی', 'انقلابیون', 'مورد')\n",
            "می*گیرند\n",
            "122\n",
            "('حواری', 'حواریون', 'مورد')\n",
            "دهیم.به\n",
            "123\n",
            "('مسئول', 'مسئولین', 'آزاده')\n",
            "غیور\n",
            "124\n",
            "('مسافر', 'مسافرین', 'طلا')\n",
            "قیمتها\n",
            "125\n",
            "('مجرم', 'مجرمین', 'شورا')\n",
            "تدویر\n",
            "126\n",
            "('انسان', 'انسانها', 'پرنده')\n",
            "گنجشکها\n",
            "127\n",
            "('ارز', 'ارزها', 'بیگانه')\n",
            "تمدنهایی\n",
            "0.0\n"
          ]
        }
      ]
    }
  ]
}